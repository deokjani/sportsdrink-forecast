# -*- coding: utf-8 -*-
'''
LSTM ì˜ˆì¸¡ ëª¨ë¸ â†’ ë¸Œëœë“œ, ì„±ë³„, ì—°ë ¹ëŒ€ë³„ë¡œ ê²€ìƒ‰ íŠ¸ë Œë“œì™€ ê¸°ìƒ ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
'''
import os
import re
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from elasticsearch import Elasticsearch
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from unidecode import unidecode

# âœ… Elasticsearch ì—°ê²° ì„¤ì •
es = Elasticsearch("http://localhost:9200")
sport_drink_index = "sports_drink_search"
weather_index = "sports_drink_weather"

# âœ… ì €ì¥í•  ê²½ë¡œ ì„¤ì •
SAVE_DIR = r"C:\ITWILL\Final_project\data\trained_models"
os.makedirs(SAVE_DIR, exist_ok=True)

# âœ… ë¸Œëœë“œ ë³€í™˜ ë”•ì…”ë„ˆë¦¬ (í•œê¸€ â†’ ì˜ì–´)
brands_mapping = {
    "íŒŒì›Œì—ì´ë“œ": "powerade",
    "ë§í‹°": "lingtea",
    "í¬ì¹´ë¦¬ìŠ¤ì›¨íŠ¸": "pocarisweat",
    "ê²Œí† ë ˆì´": "gatorade",
    "í† ë ˆíƒ€": "toreta"
}

# âœ… ë¸Œëœë“œëª… ë³€í™˜ í•¨ìˆ˜
def translate_brand_name(brand):
    return brands_mapping.get(brand, brand)  # ë”•ì…”ë„ˆë¦¬ì— ì—†ìœ¼ë©´ ì›ë˜ ê°’ ë°˜í™˜

# âœ… Scroll APIë¥¼ ì‚¬ìš©í•˜ì—¬ 25,000ê°œ ì´ìƒì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def fetch_es_data_scroll(index, start_date, end_date, scroll_size=10000):
    query = {
        "size": scroll_size,
        "query": {
            "range": {
                "period": {
                    "gte": start_date,
                    "lte": end_date
                }
            }
        }
    }
    
    # Scroll API ì´ˆê¸° ì‹¤í–‰
    res = es.search(index=index, body=query, scroll="2m")
    scroll_id = res["_scroll_id"]
    data = [hit["_source"] for hit in res["hits"]["hits"]]

    while len(res["hits"]["hits"]) > 0:
        res = es.scroll(scroll_id=scroll_id, scroll="2m")
        scroll_id = res["_scroll_id"]
        data.extend([hit["_source"] for hit in res["hits"]["hits"]])
    
    # ìŠ¤í¬ë¡¤ ì»¨í…ìŠ¤íŠ¸ ì‚­ì œ
    es.clear_scroll(scroll_id=scroll_id)

    if not data:
        print(f"âš ï¸ {index} ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
    
    return pd.DataFrame(data)

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (25,000ê°œ ì´ìƒë„ ê°€ëŠ¥)
drink_df = fetch_es_data_scroll(sport_drink_index, "2024-01-01T00:00:00.000Z", "2024-12-31T23:59:59.999Z")
weather_df = fetch_es_data_scroll(weather_index, "2024-01-01T00:00:00.000Z", "2024-12-31T23:59:59.999Z")

# âœ… ë°ì´í„° ì „ì²˜ë¦¬
def preprocess_data(drink_df, weather_df):
    for col in ["period"]:
        if col in drink_df.columns:
            drink_df["date"] = drink_df[col].apply(lambda x: x[0] if isinstance(x, list) else x)
        if col in weather_df.columns:
            weather_df["date"] = weather_df[col].apply(lambda x: x[0] if isinstance(x, list) else x)

    drink_df["date"] = pd.to_datetime(drink_df["date"], errors='coerce')
    weather_df["date"] = pd.to_datetime(weather_df["date"], errors='coerce')

    # âœ… ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë³€í™˜
    if "brand" in drink_df.columns:
        drink_df["brand"] = drink_df["brand"].apply(translate_brand_name)

    df = pd.merge(drink_df, weather_df, on="date", how="left")
    df.set_index("date", inplace=True)

    feature_cols = [col for col in df.columns if "ratios." in col] + ["temp_avg", "rainfall"]
    df = df[["brand", "age_group", "gender"] + feature_cols].dropna()

    return df, feature_cols

processed_df, feature_cols = preprocess_data(drink_df, weather_df)

# âœ… LSTM ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ë®ì–´ì“°ê¸° ê¸°ëŠ¥ ì¶”ê°€)
def train_lstm_model(df, feature_cols, save_path, seq_length=7):
    scaler_path = os.path.join(save_path, "scaler.pkl")
    model_path = os.path.join(save_path, "lstm_model.h5")

    # âœ… ê¸°ì¡´ ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì‚­ì œ (ë®ì–´ì“°ê¸°)
    if os.path.exists(model_path):
        os.remove(model_path)
        print(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ: {model_path}")
    if os.path.exists(scaler_path):
        os.remove(scaler_path)
        print(f"ğŸ”„ ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ ì‚­ì œ: {scaler_path}")

    scaler = MinMaxScaler()
    df_scaled = scaler.fit_transform(df[feature_cols])

    X, y = [], []
    for i in range(len(df_scaled) - seq_length):
        X.append(df_scaled[i:i+seq_length])
        y.append(df_scaled[i+seq_length])

    X, y = np.array(X), np.array(y)

    # âœ… ì…ë ¥ í¬ê¸° í™•ì¸
    print(f"Training data shape: X={X.shape}, y={y.shape}")

    model = Sequential([
        LSTM(128, activation='relu', return_sequences=True, input_shape=(seq_length, len(feature_cols))),
        Dropout(0.3),
        LSTM(64, activation='relu'),
        Dropout(0.3),
        Dense(len(feature_cols))
    ])

    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    model.fit(X, y, epochs=100, batch_size=32, verbose=1, validation_split=0.2, callbacks=[early_stopping])

    # âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ë®ì–´ì“°ê¸°)
    with open(scaler_path, "wb") as f:
        pickle.dump(scaler, f)
    model.save(model_path)

    print(f"âœ… {save_path} ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ!")

# âœ… í´ë”ëª… ë³€í™˜ (í•œê¸€ì„ ë¡œë§ˆìë¡œ ë³€í™˜)
def sanitize_folder_name(name):
    name = unidecode(name)  
    name = re.sub(r'[^a-zA-Z0-9_]', '_', name)  
    return name

# âœ… í•™ìŠµ ì‹¤í–‰ (ë¸Œëœë“œ, ì„±ë³„, ì—°ë ¹ëŒ€ë³„ ì €ì¥)
def train_and_save_models(df, feature_cols, seq_length=7):
    grouped = df.groupby(["brand", "age_group", "gender"])

    for (brand, age_group, gender), group in grouped:
        brand_sanitized = sanitize_folder_name(brand)
        age_group_sanitized = sanitize_folder_name(age_group)
        gender_sanitized = sanitize_folder_name(gender)
        save_path = os.path.join(SAVE_DIR, f"{brand_sanitized}_{age_group_sanitized}_{gender_sanitized}")

        if not os.path.exists(save_path):
            os.makedirs(save_path, exist_ok=True)

        print(f"ğŸ”¹ Training model for Brand: {brand}, Age Group: {age_group}, Gender: {gender}")

        train_lstm_model(group, feature_cols, save_path, seq_length)

# âœ… ì‹¤í–‰
train_and_save_models(processed_df, feature_cols, seq_length=7)

